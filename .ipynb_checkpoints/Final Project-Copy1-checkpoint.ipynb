{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on Trending Youtube Videos\n",
    "## By Alberto Espinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Problem Description\n",
    "\n",
    "   YouTube is a video-sharing website that allows users to upload, view, and share videos. There is a wide variety of content, such as music videos, gaming videos, video blogging, and documentaries. Youtube has a list of Trending videos that shows what videos are rising in popularity around the world. According to the Youtube Help page \"Trending on YouTube\", view count, the rate of growth in views, source of views, and the age of the video are some of the factors that are considered when determining if a video is trending. A video that is trending will be more accessible to viewers, resulting in more potential views and more ad revenue. Therefore, a model that predicts how long a video will be trending would be useful for content creators to maximize their trending time and views. One of the objectives is to find such a model by applying simple linear regression and multi-linear regression. Another objective is to use a regression model to predict the number of views a video will get based on certain colinear features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Set Description\n",
    "This dataset contains metadata on a large number of trending videos from different regions. Some of the features in this data set are the view count, number of likes and dislikes, number of comments, and the tags. These features will be useful when creating our model.\n",
    "For this problem, all of the data sets from each region will be concatenated into one global data set.<br>\n",
    "The data set can be found here: https://www.kaggle.com/datasnaek/youtube-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Data from CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will retrieve all of the CSV files using glob. Then, we will open each file and store it into a DataFrame object using pandas. Next, we will concatenate all of the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_US = pd.read_csv(\"USvideos.csv\")\n",
    "df_US['category_id'] = df_US['category_id'].astype(str)\n",
    "\n",
    "category_id = {}\n",
    "\n",
    "with open('US_category_id.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    for category in data['items']:\n",
    "        category_id[category['id']] = category['snippet']['title']\n",
    "\n",
    "df_US.insert(4, 'category', df_US['category_id'].map(category_id))\n",
    "category_list = df_US['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Columns of the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>category</th>\n",
       "      <th>num_tags</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5x1FAiIq_pQ</th>\n",
       "      <td>136</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Alicia Keys - When You Were Gone</td>\n",
       "      <td>Alicia Keys</td>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>15:49:21</td>\n",
       "      <td>[none]</td>\n",
       "      <td>95944</td>\n",
       "      <td>1354</td>\n",
       "      <td>181</td>\n",
       "      <td>117</td>\n",
       "      <td>https://i.ytimg.com/vi/5x1FAiIq_pQ/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Find out more in The Vault: http://bit.ly/AK_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>htvR_dBs3eg</th>\n",
       "      <td>127</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Sam Smith - The Thrill of It All ALBUM REVIEW</td>\n",
       "      <td>theneedledrop</td>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>Music</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>21:38:57</td>\n",
       "      <td>album|\"review\"|\"music\"|\"reviews\"|\"indie\"|\"unde...</td>\n",
       "      <td>98422</td>\n",
       "      <td>2926</td>\n",
       "      <td>106</td>\n",
       "      <td>798</td>\n",
       "      <td>https://i.ytimg.com/vi/htvR_dBs3eg/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Listen: https://www.youtube.com/watch?v=J_ub7E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vd4zwINEcLY</th>\n",
       "      <td>139</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Live in the now!</td>\n",
       "      <td>poofables</td>\n",
       "      <td>2011-03-27</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>04:31:25</td>\n",
       "      <td>cash|\"Wayne's\"|\"World\"|\"wayne\"|\"waynes\"|\"fende...</td>\n",
       "      <td>95085</td>\n",
       "      <td>909</td>\n",
       "      <td>52</td>\n",
       "      <td>193</td>\n",
       "      <td>https://i.ytimg.com/vi/vd4zwINEcLY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Stop torturing yourself man, you'll never affo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7fm7mll2qvg</th>\n",
       "      <td>140</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Sigrid - Strangers (Lyric Video)</td>\n",
       "      <td>SigridVEVO</td>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>Music</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>Sigrid|\"Strangers\"|\"Island\"|\"Records\"|\"Pop\"</td>\n",
       "      <td>91776</td>\n",
       "      <td>4604</td>\n",
       "      <td>46</td>\n",
       "      <td>357</td>\n",
       "      <td>https://i.ytimg.com/vi/7fm7mll2qvg/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Listen to Strangers here: https://Sigrid.lnk.t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q-WipZ9p0wk</th>\n",
       "      <td>143</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Three meals that cost me $1.50 each</td>\n",
       "      <td>Brothers Green Eats</td>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>14:00:08</td>\n",
       "      <td>brothers green eats|\"budget cooking\"|\"cooking ...</td>\n",
       "      <td>77630</td>\n",
       "      <td>1991</td>\n",
       "      <td>83</td>\n",
       "      <td>208</td>\n",
       "      <td>https://i.ytimg.com/vi/q-WipZ9p0wk/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Welcome to day three of cooking for the price ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index trending_date  \\\n",
       "video_id                           \n",
       "5x1FAiIq_pQ    136    2017-11-14   \n",
       "htvR_dBs3eg    127    2017-11-14   \n",
       "vd4zwINEcLY    139    2017-11-14   \n",
       "7fm7mll2qvg    140    2017-11-14   \n",
       "q-WipZ9p0wk    143    2017-11-14   \n",
       "\n",
       "                                                     title  \\\n",
       "video_id                                                     \n",
       "5x1FAiIq_pQ               Alicia Keys - When You Were Gone   \n",
       "htvR_dBs3eg  Sam Smith - The Thrill of It All ALBUM REVIEW   \n",
       "vd4zwINEcLY                               Live in the now!   \n",
       "7fm7mll2qvg               Sigrid - Strangers (Lyric Video)   \n",
       "q-WipZ9p0wk            Three meals that cost me $1.50 each   \n",
       "\n",
       "                   channel_title publish_date       category  num_tags  \\\n",
       "video_id                                                                 \n",
       "5x1FAiIq_pQ          Alicia Keys   2017-11-09          Music         1   \n",
       "htvR_dBs3eg        theneedledrop   2017-11-10          Music        31   \n",
       "vd4zwINEcLY            poofables   2011-03-27  Entertainment         9   \n",
       "7fm7mll2qvg           SigridVEVO   2017-11-10          Music         5   \n",
       "q-WipZ9p0wk  Brothers Green Eats   2017-11-09  Howto & Style        19   \n",
       "\n",
       "            category_id publish_time  \\\n",
       "video_id                               \n",
       "5x1FAiIq_pQ          10     15:49:21   \n",
       "htvR_dBs3eg          10     21:38:57   \n",
       "vd4zwINEcLY          24     04:31:25   \n",
       "7fm7mll2qvg          10     00:00:00   \n",
       "q-WipZ9p0wk          26     14:00:08   \n",
       "\n",
       "                                                          tags  views  likes  \\\n",
       "video_id                                                                       \n",
       "5x1FAiIq_pQ                                             [none]  95944   1354   \n",
       "htvR_dBs3eg  album|\"review\"|\"music\"|\"reviews\"|\"indie\"|\"unde...  98422   2926   \n",
       "vd4zwINEcLY  cash|\"Wayne's\"|\"World\"|\"wayne\"|\"waynes\"|\"fende...  95085    909   \n",
       "7fm7mll2qvg        Sigrid|\"Strangers\"|\"Island\"|\"Records\"|\"Pop\"  91776   4604   \n",
       "q-WipZ9p0wk  brothers green eats|\"budget cooking\"|\"cooking ...  77630   1991   \n",
       "\n",
       "             dislikes  comment_count  \\\n",
       "video_id                               \n",
       "5x1FAiIq_pQ       181            117   \n",
       "htvR_dBs3eg       106            798   \n",
       "vd4zwINEcLY        52            193   \n",
       "7fm7mll2qvg        46            357   \n",
       "q-WipZ9p0wk        83            208   \n",
       "\n",
       "                                             thumbnail_link  \\\n",
       "video_id                                                      \n",
       "5x1FAiIq_pQ  https://i.ytimg.com/vi/5x1FAiIq_pQ/default.jpg   \n",
       "htvR_dBs3eg  https://i.ytimg.com/vi/htvR_dBs3eg/default.jpg   \n",
       "vd4zwINEcLY  https://i.ytimg.com/vi/vd4zwINEcLY/default.jpg   \n",
       "7fm7mll2qvg  https://i.ytimg.com/vi/7fm7mll2qvg/default.jpg   \n",
       "q-WipZ9p0wk  https://i.ytimg.com/vi/q-WipZ9p0wk/default.jpg   \n",
       "\n",
       "             comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
       "video_id                                                                   \n",
       "5x1FAiIq_pQ              False             False                   False   \n",
       "htvR_dBs3eg              False             False                   False   \n",
       "vd4zwINEcLY              False             False                   False   \n",
       "7fm7mll2qvg              False             False                   False   \n",
       "q-WipZ9p0wk              False             False                   False   \n",
       "\n",
       "                                                   description  \n",
       "video_id                                                        \n",
       "5x1FAiIq_pQ  Find out more in The Vault: http://bit.ly/AK_A...  \n",
       "htvR_dBs3eg  Listen: https://www.youtube.com/watch?v=J_ub7E...  \n",
       "vd4zwINEcLY  Stop torturing yourself man, you'll never affo...  \n",
       "7fm7mll2qvg  Listen to Strangers here: https://Sigrid.lnk.t...  \n",
       "q-WipZ9p0wk  Welcome to day three of cooking for the price ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_US['trending_date'] = pd.to_datetime(df_US['trending_date'],errors='coerce', format='%y.%d.%m')\n",
    "df_US['publish_time'] = pd.to_datetime(df_US['publish_time'], errors='coerce', format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "df_US = df_US[df_US['trending_date'].notnull()]\n",
    "df_US = df_US[df_US['publish_time'].notnull()]\n",
    "\n",
    "df_US = df_US.dropna(how='any',inplace=False, axis = 0)\n",
    "\n",
    "df_US.insert(4, 'publish_date', df_US['publish_time'].dt.date)\n",
    "df_US['publish_time'] = df_US['publish_time'].dt.time\n",
    "\n",
    "num_tags = [len(list(i.split('|'))) for i in df_US['tags']]\n",
    "df_US.insert(6, 'num_tags', num_tags)\n",
    "df_US_full = df_US.reset_index().set_index('video_id')\n",
    "df_US = df_US.reset_index().sort_values('trending_date').drop_duplicates('video_id',keep='last').set_index('video_id')\n",
    "df_US.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending Columns to Dataframe\n",
    "\n",
    "We can use certain data such as the trending date and publish time to get even more data that will help us in our predictions. We can get the time between when the video was published and when the video was trending by subtracting those two columns. We can also find how long a video has been trending by finding the last date it went trending and subtracting it by the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "publish_to_trend = {}\n",
    "df_US_first = df_US_full.reset_index().drop_duplicates('video_id',keep ='first').set_index('video_id')\n",
    "diff_first = (df_US_first['trending_date']).astype('datetime64[ns]')-df_US_first['publish_date'].astype('datetime64[ns]')\n",
    "\n",
    "diff_first = diff_first.reset_index()\n",
    "diff_first.columns = ['video_id','publish_to_trend']\n",
    "\n",
    "for i, row in diff_first.iterrows():\n",
    "    publish_to_trend[row['video_id']] = row['publish_to_trend'].days\n",
    "\n",
    "df_US_last = df_US\n",
    "diff_last = df_US['trending_date'].astype('datetime64[ns]')-df_US['publish_date'].astype('datetime64[ns]')\n",
    "diff_last = diff_last.reset_index()\n",
    "diff_last.columns = ['video_id','publish_to_trend_last']\n",
    "df_US = df_US.reset_index()\n",
    "df_US.insert(4,'publish_to_trend_last', diff_last['publish_to_trend_last'].astype('timedelta64[D]').astype(int))\n",
    "df_US.insert(4, 'publish_to_trend', df_US['video_id'].map(publish_to_trend))\n",
    "df_US.insert(4, 'trend_duration', 0)\n",
    "df_US['trend_duration'] = (df_US['publish_to_trend_last']-df_US['publish_to_trend'])+1\n",
    "df_US.set_index('video_id')[['publish_to_trend','trend_duration']].sort_values('trend_duration',ascending=False).head()\n",
    "df_US = df_US.set_index('video_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predicting the Number of Times a Video is Trending\n",
    "As mentioned earlier, the view count and the age of the video are some of the factors that are considered when finding trending videos. Therefore, these features can be useful in predicting how long a video is trending. Our target will be the trending duration, and our features will be the views, likes, dislikes, comment count, number of tags, and age of the video. First, we will plot each of the features individually with respect to the trend duration to see if there is some correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5446"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(df_US['trend_duration']).astype(np.float).astype(np.int)\n",
    "X = np.array([[df_US[col][vid_id] for vid_id in df_US.index]\n",
    "              for col in ['views', 'likes', 'dislikes', 'comment_count', 'num_tags', 'publish_to_trend']]).transpose().astype(np.float).astype(np.int)\n",
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "Contrary to what the YouTube Help page suggests, there does not seem to be any pattern or relationship between the individual features and the trending duration. We will still use simple linear regression to train a section of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  Rsq=0.039281\n",
      " 1  Rsq=0.033122\n",
      " 2  Rsq=0.002263\n",
      " 3  Rsq=0.012399\n",
      " 4  Rsq=0.000787\n",
      " 5  Rsq=0.004756\n"
     ]
    }
   ],
   "source": [
    "natt = X.shape[1]\n",
    "ym = np.mean(y)\n",
    "syy = np.mean((y-ym)**2)\n",
    "Rsq = np.zeros(natt)\n",
    "beta0 = np.zeros(natt)\n",
    "beta1 = np.zeros(natt)\n",
    "for k in range(natt):\n",
    "    xm = np.mean(X[:,k])\n",
    "    sxy = np.mean((X[:,k]-xm)*(y-ym))\n",
    "    sxx = np.mean((X[:,k]-xm)**2)\n",
    "    beta1[k] = sxy/sxx\n",
    "    beta0[k] = ym - beta1[k]*xm\n",
    "    y_pred_k = beta0[k]+beta1[k]*X[:,k]\n",
    "    Rsq[k] = 1 - np.mean((y-y_pred_k)**2)/np.mean((y-ym)**2)\n",
    "       \n",
    "    print(\"{0:2d}  Rsq={1:f}\".format(k,Rsq[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression\n",
    "Unfortunately, based on the R squared values calculated above, each of the models yields a low R squared value, meaning a small portion of the variance is explained by the model. We can use multiple features to improve our results by using multilinear regression. Here, we will create our data matrix with our features as column vectors. We will then use scikit's LinearRegression to fit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "n_examples = X.shape[0]\n",
    "ne_train = 4000\n",
    "ne_test = n_examples\n",
    "X_tr = X[:ne_train,:].astype(np.float).astype(np.int)\n",
    "y_tr = y[:ne_train].astype(np.float).astype(np.int)\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_tr,y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see how well our training set did by calculating R squared, and we will test our training data on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.052287\n",
      "R^2 = -0.475071\n"
     ]
    }
   ],
   "source": [
    "ym = np.mean(y_tr) # recomputing it \n",
    "y_tr_pred = regr.predict(X_tr)\n",
    "RSS_tr = np.sum((y_tr_pred-y_tr)**2)\n",
    "TSS_tr = np.sum((y_tr-ym)**2)\n",
    "Rsq_tr = 1- RSS_tr/TSS_tr\n",
    "\n",
    "print(\"R^2 = {0:f}\".format(Rsq_tr))\n",
    "\n",
    "X_test = X[ne_train:,:]\n",
    "y_test = y[ne_train:]\n",
    "ym_test = np.mean(y_test)\n",
    "y_test_pred = regr.predict(X_test)\n",
    "RSS_test = np.sum((y_test_pred-y_test)**2)\n",
    "TSS_test = np.sum((y_test -ym_test )**2)\n",
    "Rsq_test = 1-RSS_test/TSS_test\n",
    "\n",
    "print(\"R^2 = {0:f}\".format(Rsq_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data\n",
    "Since the feature scales vary greatly, it may take longer for the algorithm to reach the global minimum. Therefore, normalization could help improve the results and the convergence speed of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.052287\n",
      "R^2 = -0.475071\n"
     ]
    }
   ],
   "source": [
    "regr_norm = linear_model.LinearRegression(normalize=True)\n",
    "regr_norm.fit(X_tr, y_tr)\n",
    "\n",
    "ym = np.mean(y_tr)\n",
    "y_tr_pred = regr.predict(X_tr)\n",
    "RSS_tr = np.sum((y_tr_pred-y_tr)**2)\n",
    "TSS_tr = np.sum((y_tr-ym)**2)\n",
    "Rsq_tr = 1- RSS_tr/TSS_tr\n",
    "\n",
    "print(\"R^2 = {0:f}\".format(Rsq_tr))\n",
    "\n",
    "X_test = X[ne_train:,:]\n",
    "y_test = y[ne_train:]\n",
    "ym_test = np.mean(y_test)\n",
    "y_test_pred = regr.predict(X_test)\n",
    "RSS_test = np.sum((y_test_pred-y_test)**2)\n",
    "TSS_test = np.sum((y_test -ym_test )**2)\n",
    "Rsq_test = 1-RSS_test/TSS_test\n",
    "\n",
    "print(\"R^2 = {0:f}\".format(Rsq_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting View Count Using Multilinear Regression\n",
    "For this regression problem, the target is the number of views a video gets, and the features are the likes, dislikes, comment count, and number of tags.\n",
    "The target and data matrices will be set up accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array(df_US['views']).astype(np.float).astype(np.int)\n",
    "X = np.array([df_US[col] for col in ['likes', 'dislikes', 'comment_count', 'num_tags']]).transpose().astype(np.float).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   32.6998767 ,    72.52986005,   -90.42517289,  2483.53222395])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_examples = X.shape[0]\n",
    "ne_train = 4000\n",
    "ne_test = n_examples\n",
    "X_tr = X[:ne_train,:].astype(np.float).astype(np.int)\n",
    "y_tr = y[:ne_train].astype(np.float).astype(np.int)\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_tr,y_tr)\n",
    "regr_norm = linear_model.LinearRegression(normalize=True)\n",
    "regr_norm.fit(X_tr, y_tr)\n",
    "regr_norm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.786837\n"
     ]
    }
   ],
   "source": [
    "ym = np.mean(y_tr)\n",
    "y_tr_pred = regr.predict(X_tr)\n",
    "RSS_tr = np.sum((y_tr_pred-y_tr)**2)\n",
    "TSS_tr = np.sum((y_tr-ym)**2)\n",
    "Rsq_tr = 1- RSS_tr/TSS_tr\n",
    "\n",
    "print(\"R^2 = {0:f}\".format(Rsq_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.786837\n",
      "R^2 = 0.640657\n"
     ]
    }
   ],
   "source": [
    "ym = np.mean(y_tr) # recomputing it \n",
    "y_tr_pred = regr_norm.predict(X_tr)\n",
    "RSS_tr = np.sum((y_tr_pred-y_tr)**2)\n",
    "TSS_tr = np.sum((y_tr-ym)**2)\n",
    "Rsq_tr = 1- RSS_tr/TSS_tr\n",
    "\n",
    "print(\"R^2 = {0:f}\".format(Rsq_tr))\n",
    "\n",
    "X_test = X[ne_train:,:]\n",
    "y_test = y[ne_train:]\n",
    "ym_test = np.mean(y_test)\n",
    "y_test_pred = regr.predict(X_test)\n",
    "RSS_test = np.sum((y_test_pred-y_test)**2)\n",
    "TSS_test = np.sum((y_test -ym_test )**2)\n",
    "Rsq_test = 1-RSS_test/TSS_test\n",
    "\n",
    "print(\"R^2 = {0:f}\".format(Rsq_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the R squared values calculated above, this model explains a significant portion of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting View Count using Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "scaler.transform(X_tr)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', C=1e3)\n",
    "svr_poly = SVR(kernel='poly', C=1e3, degree=3)\n",
    "\n",
    "y_hat_rbf = svr_rbf.fit(X_tr, y_tr).predict(X_test)\n",
    "y_hat_lin = svr_lin.fit(X_tr, y_tr).predict(X_test)\n",
    "y_hat_poly = svr_poly.fit(X_tr, y_tr).predict(X_test)\n",
    "\n",
    "ym_test = np.mean(y_test)\n",
    "\n",
    "RSS_rbf_test = np.sum((y_hat_rbf-y_test)**2)\n",
    "TSS_rbf_test = np.sum((y_test-ym_test )**2)\n",
    "Rsq_rbf_test = 1-RSS_rbf_test/TSS_rbf_test\n",
    "\n",
    "RSS_lin_test = np.sum((y_hat_lin-y_test)**2)\n",
    "TSS_lin_test = np.sum((y_test-ym_test )**2)\n",
    "Rsq_lin_test = 1-RSS_lin_test/TSS_lin_test\n",
    "\n",
    "RSS_poly_test = np.sum((y_hat_poly-y_test)**2)\n",
    "TSS_poly_test = np.sum((y_test -ym_test )**2)\n",
    "Rsq_poly_test = 1-RSS_poly_test/TSS_poly_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting View Count Using a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "rand_seed = 5\n",
    "numpy.random.seed(rand_seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=base_model, \n",
    "                                         epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=rand_seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Video Category Using a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_US['category_id'] = df_US['category_id'].astype(str)\n",
    "\n",
    "category_id = {}\n",
    "\n",
    "with open('US_category_id.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    for category in data['items']:\n",
    "        category_id[category['id']] = category['snippet']['title']\n",
    "\n",
    "df_US.insert(4, 'category', df_US['category_id'].map(category_id))\n",
    "category_list = df_US['category'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Video Clustering\n",
    "Many of the ads that run on YouTube are targeted towards a specific audience. For instance, gamers who watch gaming videos on YouTube would recieve advertisements about gaming-related products such as headsets or gaming consoles. YouTube would need to find a way to categorize these viewers based on the videos they watch in order to cater these advertisements to the right audience. Although YouTube already categorizes its videos, this problem might require different categories. K-means clustering can be used to solve this problem by using bag-of-words approach to separate videos into different categories based on their description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from time import time\n",
    "\n",
    "df_US = pd.read_csv(\"USvideos.csv\")\n",
    "df_US['category_id'] = df_US['category_id'].astype(str)\n",
    "\n",
    "category_id = {}\n",
    "\n",
    "with open('US_category_id.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    for category in data['items']:\n",
    "        category_id[category['id']] = category['snippet']['title']\n",
    "\n",
    "df_US.insert(4, 'category', df_US['category_id'].map(category_id))\n",
    "category_list = df_US['category'].unique()\n",
    "\n",
    "rand_seed = 7\n",
    "numpy.random.seed(rand_seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57883 video descriptions\n",
      "16 categories\n",
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "Finished vectorizing in 6.078281s\n",
      "# Samples: 30750, # Features: 55144\n",
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=16, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0)\n",
      "Finished clustering in 58.201s\n",
      "Homogeneity: 0.123\n",
      "Completeness: 0.172\n",
      "V-measure: 0.144\n",
      "Adjusted Rand-Index: 0.008\n",
      "Silhouette Coefficient: 0.030\n",
      "Top terms per cluster:\n",
      "Cluster 0: idol american callofduty americanidol markiplier treyarch abcnetwork abc reveal dunkey\n",
      "Cluster 1: bit ly late cbs code nhttp mythical video use youtube\n",
      "Cluster 2: smarturl iqid yt nhttp eazy music available napple namazon nspotify\n",
      "Cluster 3: nba cut news firstwefeast nbcnews nbc tmz feast po itsalexclark\n",
      "Cluster 4: nbc jimmy nnbc kimmel night snl tumblr tonight nbcsnl late\n",
      "Cluster 5: nhttp youtube video nfollow ly new ntwitter ninstagram bit nfacebook\n",
      "Cluster 6: gl goo wwe video nsubscribe nvisit youtube nfollow videos nhttps\n",
      "Cluster 7: nan 힐링쿠킹이에요 finance finding finder finchittida finch finbarr financing financier\n",
      "Cluster 8: thisisinsider ninsider bon insider appétit appetit believes adventure kitchen food\n",
      "Cluster 9: nhttps youtube nhttp buzzfeed videos refinery29 watch list youtu nlicensed\n",
      "Cluster 10: amzn nhttp hellthyjunkfood mkbhd ijustine nsony gracehelbig use bit camera\n",
      "Cluster 11: nfl espn pn es youtube follow football highlights nnfl nba\n",
      "Cluster 12: youtube list watch licensing lelepons anwar hannahstocking nshots rudymancuso lizakoshy\n",
      "Cluster 13: williamosman nextbeat nvia simonegiertz patreon incompetech myvirginkitchen audio williamosmanscience crabsandscience\n",
      "Cluster 14: netflix marvel cbs bit ly morning news members nfollow nlike\n",
      "Cluster 15: ni nand know tomscott nyou don just nwe gonna love\n"
     ]
    }
   ],
   "source": [
    "print(\"%d video descriptions\" % df_US.shape[0])\n",
    "print(\"%d categories\" % len(category_list))\n",
    "\n",
    "labels = df_US['category']\n",
    "true_k = len(category_list)\n",
    "\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "init_t = time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(df_US['description'].astype(str))\n",
    "\n",
    "print(\"Finished vectorizing in %fs\" % (time() - init_t))\n",
    "print(\"# Samples: %d, # Features: %d\" % X.shape)\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "init_t = time()\n",
    "km.fit(X)\n",
    "print(\"Finished clustering in %0.3fs\" % (time() - init_t))\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an idea of how well the unsupervised model did by observing the homogeneity and the completeness. The homogeneity score is dependant on how how separated the clusters are. This model's homogeneity was close to zero, meaning that the clusters were not separated well and contained data points from other classes.<br>\n",
    "The completeness score is dependent on how many data points in a certain class are in the same cluster. A low completness score means that the data points blended into different clusters.\n",
    "The low scores can be attributed to the structure of the descriptions. Many of them contained links to other webpages, which were incorrectly parsed when vectorized and filled the matrix with useless information. Also, many of the words were not defined in the dictionary, such as channel names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
